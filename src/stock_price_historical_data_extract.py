# -*- coding: utf-8 -*-
"""stock_price_historical_data_extract.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13poeXxwYkVV3S1cHADj-RXPS3SgpMSJB
"""

# Import and pre-process 2023 data of DSE
import os
import pandas as pd

# Path to your CSV files
csv_folder_path = '/content/drive/My Drive/BISS/EMTH0009 - Master Thesis/Data/DSE_data_2023'

# Read CSV files and store in a dictionary
DSE_dict_2023 = {}
for filename in os.listdir(csv_folder_path):
    if filename.endswith('.csv'):
        # Extract date from filename
        date = filename.split('_')[-1].split('.')[0]
        # Read CSV file into DataFrame
        df = pd.read_csv(os.path.join(csv_folder_path, filename))
        # Add DataFrame to dictionary with date as key
        DSE_dict_2023[date] = df

# Example: Access DataFrame of DSE for a specific date
date = '20230101'  # Change this to the date you want
if date in DSE_dict_2023:
    print("Data for", date)
    print(DSE_dict_2023[date])
else:
    print("Data for", date, "not found")

# Keep data of only indices DS30, DSEX and 30 stocks in DS30 index
# List of desired stock codes
DS30_stocks = [
    'IFIC',
    'BATBC',
    'BEACONPHAR',
    'BEXIMCO',
    'ORIONPHARM',
    'BRACBANK',
    'BSC',
    'DELTALIFE',
    'BSRMLTD',
    'POWERGRID',
    'FORTUNE',
    'GP',
    'BSCCL',
    'ROBI',
    'SOUTHEASTB',
    'ISLAMIBANK',
    'BBSCABLES',
    'BXPHARMA',
    'LHBL',
    'CITYBANK',
    'MPETROLEUM',
    'OLYMPIC',
    'GPHISPAT',
    'RENATA',
    'SEAPEARL',
    'TITASGAS',
    'SQURPHARMA',
    'UPGDCL',
    'UNIQUEHRL',
    'IDLC','00DS30','00DSEX'
]

# Iterate over dataframes and filter rows
for date, df in DSE_dict_2023.items():
    # Keep rows with desired stock codes
    df = df[df['Scrip'].isin(DS30_stocks)]
    # Update dataframe in dictionary
    DSE_dict_2023[date] = df

# Recheck dataframe to see if non-neccesary stocks have been removed
date = '20230101'  # Change this to the date you want
if date in DSE_dict_2023:
    print("Data for", date)
    print(DSE_dict_2023[date])
else:
    print("Data for", date, "not found")

# Concatenate all DataFrames in the dictionary
dse_df_2023 = pd.concat(DSE_dict_2023.values(), ignore_index=True)

# Print the resulting DataFrame
print(dse_df_2023)
dse_df_2023.to_csv('/content/drive/My Drive/BISS/EMTH0009 - Master Thesis/Data/dse_df_2023.csv', index=False)

pip install xlwt
import os
import requests
import pandas as pd
# Make a scraping code to extract stock price data for VN market in 2019
def scrape_and_save_to_excel(symbol, start_date, end_date, page_index=1, page_size=365):
    # API endpoint URL
    api_url = f"https://s.cafef.vn/Ajax/PageNew/DataHistory/PriceHistory.ashx?Symbol={symbol}&StartDate={start_date}&EndDate={end_date}&PageIndex={page_index}&PageSize={page_size}"

    # Send HTTP GET request to the API
    response = requests.get(api_url)

    # Check if request was successful
    if response.status_code == 200:
        # Parse JSON response
        data = response.json()

        # Extract historical price data
        historical_data = data['Data']['Data']

        # Create DataFrame
        df = pd.DataFrame(historical_data)

        # Define target directory
        target_directory = '/content/drive/My Drive/BISS/EMTH0009 - Master Thesis/Data/HOSE_data_2019'

        # Create target directory if it doesn't exist
        os.makedirs(target_directory, exist_ok=True)

        # Save DataFrame to Excel file in target directory
        # Replace slashes with underscores in the filename
        excel_filename = f"{symbol}_{start_date.replace('/', '_')}_{end_date.replace('/', '_')}.xls"
        excel_filepath = os.path.join(target_directory, excel_filename)
        df.to_excel(excel_filepath, index=False)
        print(f"Excel file saved: {excel_filepath}")
    else:
        print(f"Failed to retrieve data. Status code: {response.status_code}")

# Run iteration through all VN30 stocks and 2 indices
tickers = [
    'CII', 'CTD', 'CTG', 'DHG', 'DPM', 'EIB', 'FPT', 'GAS', 'GMD', 'HDB',
    'HPG', 'MBB', 'MSN', 'MWG', 'NVL', 'PNJ', 'REE', 'ROS', 'SAB', 'SBT',
    'SSI', 'STB', 'TCB', 'VCB', 'VHM', 'VIC', 'VJC', 'VNM', 'VPB', 'VRE',
    'VNINDEX', 'VN30INDEX'
]

start_date = '01/01/2019'
end_date = '12/31/2019'

for symbol in tickers:
    scrape_and_save_to_excel(symbol, start_date, end_date)

# Make a scraping code to extract stock price data for VN market in 2023
def scrape_and_save_to_excel(symbol, start_date, end_date, page_index=1, page_size=365):
    # API endpoint URL
    api_url = f"https://s.cafef.vn/Ajax/PageNew/DataHistory/PriceHistory.ashx?Symbol={symbol}&StartDate={start_date}&EndDate={end_date}&PageIndex={page_index}&PageSize={page_size}"

    # Send HTTP GET request to the API
    response = requests.get(api_url)

    # Check if request was successful
    if response.status_code == 200:
        # Parse JSON response
        data = response.json()

        # Extract historical price data
        historical_data = data['Data']['Data']

        # Create DataFrame
        df = pd.DataFrame(historical_data)

        # Define target directory
        target_directory = '/content/drive/My Drive/BISS/EMTH0009 - Master Thesis/Data/HOSE_data_2023'

        # Create target directory if it doesn't exist
        os.makedirs(target_directory, exist_ok=True)

        # Save DataFrame to Excel file in target directory
        # Replace slashes with underscores in the filename
        excel_filename = f"{symbol}_{start_date.replace('/', '_')}_{end_date.replace('/', '_')}.xls"
        excel_filepath = os.path.join(target_directory, excel_filename)
        df.to_excel(excel_filepath, index=False)
        print(f"Excel file saved: {excel_filepath}")
    else:
        print(f"Failed to retrieve data. Status code: {response.status_code}")

# Run iteration through all VN30 stocks and 2 indices
tickers = [
    'ACB', 'BCM', 'BID', 'BVH', 'CTG', 'FPT', 'GAS', 'GVR', 'HDB', 'HPG',
    'MBB', 'MSN', 'MWG', 'NVL', 'PDR', 'PLX', 'POW', 'SAB', 'SSI', 'STB',
    'TCB', 'TPB', 'VCB', 'VHM', 'VIB', 'VIC', 'VJC', 'VNM', 'VPB', 'VRE',
    'VNINDEX', 'VN30INDEX'
]

start_date = '01/01/2023'
end_date = '12/31/2023'

for symbol in tickers:
    scrape_and_save_to_excel(symbol, start_date, end_date)

# Import 2019 data of HOSE and pre-process
import os
import pandas as pd

# Function to process each file
def process_file(file_path):
    # Read the file into a DataFrame
    df = pd.read_excel(file_path)

    # Remove specified columns
    df.drop(columns=['GiaDieuChinh', 'ThayDoi', 'GiaTriKhopLenh', 'GtThoaThuan'], inplace=True)

    # Extract ticker from file name
    ticker = os.path.basename(file_path).split('_')[0]

    # Add 'ticker' column
    df.insert(1, 'ticker', ticker)

    # Calculate 'volume' and add as a new column
    df['volume'] = df['KhoiLuongKhopLenh'] + df['KLThoaThuan']

    # Remove columns 'KhoiLuongKhopLenh' and 'KLThoaThuan'
    df.drop(columns=['KhoiLuongKhopLenh', 'KLThoaThuan'], inplace=True)

    # Rename columns
    df.rename(columns={'Ngay': 'date', 'GiaDongCua': 'close', 'GiaMoCua': 'open', 'GiaCaoNhat': 'high', 'GiaThapNhat': 'low'}, inplace=True)

    return df

# Path to the folder containing the files
folder_path = '/content/drive/My Drive/BISS/EMTH0009 - Master Thesis/Data/HOSE_data_2019/'

# List to store DataFrames
dfs = []

# Iterate through each file in the folder
for file_name in os.listdir(folder_path):
    if file_name.endswith('.xls'):
        file_path = os.path.join(folder_path, file_name)
        # Process each file and append the DataFrame to the list
        dfs.append(process_file(file_path))

# Concatenate all DataFrames into a single DataFrame
hose_df_2019 = pd.concat(dfs, ignore_index=True)

# Print the resulting DataFrame hose_df_2019 to check
print(hose_df_2019.head(n = 20))
print(hose_df_2019.tail(n = 20))

# Export cleaned data to another .csv file for backup
hose_df_2019.to_csv('/content/drive/My Drive/BISS/EMTH0009 - Master Thesis/Data/hose_df_2019.csv', index=False)

# Re run the importing and pre-processing with data of 2023, HOSE
# Path to the folder containing the files
folder_path = '/content/drive/My Drive/BISS/EMTH0009 - Master Thesis/Data/HOSE_data_2023/'

# List to store DataFrames
dfs = []

# Iterate through each file in the folder
for file_name in os.listdir(folder_path):
    if file_name.endswith('.xls'):
        file_path = os.path.join(folder_path, file_name)
        # Process each file and append the DataFrame to the list
        dfs.append(process_file(file_path))

# Concatenate all DataFrames into a single DataFrame
hose_df_2023 = pd.concat(dfs, ignore_index=True)

# Print the resulting DataFrame hose_df_2023 to check
print(hose_df_2023.head(n = 20))
print(hose_df_2023.tail(n = 20))

# Export cleaned data to another .csv file for backup
hose_df_2023.to_csv('/content/drive/My Drive/BISS/EMTH0009 - Master Thesis/Data/hose_df_2023.csv', index=False)